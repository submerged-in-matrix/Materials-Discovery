{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d013ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = os.getenv(\"MP_API_KEY\")\n",
    "\n",
    "if \"MP_API_KEY\" not in os.environ:\n",
    "    os.environ[\"MP_API_KEY\"] = getpass(\"Enter your MP_API_KEY (hidden input): \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b25a3",
   "metadata": {},
   "source": [
    "# Data parsing\n",
    "**Miniature semiconductor systems**  \n",
    "**Steps (via MP-api)**  \n",
    "*Setting API → select FIELDS (incl. structure, use nelements field) → define chemistry menus (III–V, II–VI, group-IV binaries; III–V–N, III–V–VI ternaries)*  \n",
    "*→ build pairs/ternaries (dedup, guarantee ternaries are triples) → fetch helper (materials.summary.search(elements=..., num_elements=..., chunk_size, num_chunks=1); suppress progress)*  \n",
    "*→ collect (loop binaries → then ternaries until TARGET_N; keep only 0<Eg<4 eV; track seen by material_id) → serialize (JSON.gz full: structure.as_dict(), Element→symbol; CSV.gz comp-only)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ada5c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1285 unique semiconductor-like entries (binaries/ternaries).\n",
      "✅ saved JSON → data/semiconductors_full_with_struct.json.gz (records: 1285)\n",
      "✅ saved CSV  → data/semiconductors_comp_only.csv.gz (rows: 1285)\n",
      "(1285, 10)\n",
      "  material_id formula_pretty elements  nelements  formation_energy_per_atom  \\\n",
      "0  mp-1244872             BN   [B, N]          2                  -0.771438   \n",
      "1  mp-1244917             BN   [B, N]          2                  -0.496787   \n",
      "2  mp-1244943             BN   [B, N]          2                  -0.756250   \n",
      "\n",
      "   energy_above_hull  is_stable  band_gap  is_metal   density  \n",
      "0           0.641541      False    1.5766     False  1.574835  \n",
      "1           0.916192      False    1.1992     False  1.588014  \n",
      "2           0.656729      False    1.1885     False  1.708090  \n"
     ]
    }
   ],
   "source": [
    "# setting env var outside Python to silence tqdm globally (preferred)\n",
    "# Windows (CMD):   set DISABLE_TQDM=1\n",
    "# PowerShell:      $env:DISABLE_TQDM=\"1\"\n",
    "# macOS/Linux:     export DISABLE_TQDM=1\n",
    "\n",
    "import os, sys, gzip, json, itertools, contextlib\n",
    "import pandas as pd\n",
    "from mp_api.client import MPRester\n",
    "\n",
    "# ----------------------------\n",
    "# config\n",
    "# ----------------------------\n",
    "\n",
    "INCLUDE_STRUCTURE = True\n",
    "TARGET_N = 10_000\n",
    "EG_MIN, EG_MAX = 0.0, 4.0     # setting semiconductor-ish window\n",
    "\n",
    "# setting fields (include 'structure' for structure-based pipeline)\n",
    "# note: 'nelements' is a FIELD on docs; 'num_elements' is the FILTER param\n",
    "FIELDS = [\n",
    "    \"material_id\", \"formula_pretty\", \"elements\", \"nelements\",\n",
    "    \"formation_energy_per_atom\", \"energy_above_hull\", \"is_stable\",\n",
    "    \"band_gap\", \"is_metal\", \"density\"\n",
    "]\n",
    "if INCLUDE_STRUCTURE:\n",
    "    FIELDS.append(\"structure\")\n",
    "\n",
    "# setting element families commonly seen in semiconductors\n",
    "group_IV = [\"C\",\"Si\",\"Ge\",\"Sn\"]\n",
    "III      = [\"B\",\"Al\",\"Ga\",\"In\"]\n",
    "V        = [\"N\",\"P\",\"As\",\"Sb\",\"Bi\"]\n",
    "II       = [\"Zn\",\"Cd\",\"Hg\",\"Mg\",\"Be\",\"Ca\",\"Sr\",\"Ba\"]\n",
    "VI       = [\"O\",\"S\",\"Se\",\"Te\"]\n",
    "halides  = [\"F\",\"Cl\",\"Br\",\"I\"]  # available if you want I–VII\n",
    "\n",
    "# ----------------------------\n",
    "# utilities\n",
    "# ----------------------------\n",
    "\n",
    "# setting a context manager to suppress tqdm/progress output from mp_api calls\n",
    "@contextlib.contextmanager\n",
    "def suppress_progress():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_out, old_err = sys.stdout, sys.stderr\n",
    "        try:\n",
    "            sys.stdout, sys.stderr = devnull, devnull\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_out, old_err\n",
    "\n",
    "# setting builders for pairs and ternaries\n",
    "def build_pairs():\n",
    "    pairs = []\n",
    "    pairs += list(itertools.product(III, V))            # III–V\n",
    "    pairs += list(itertools.product(II, VI))            # II–VI\n",
    "    pairs += list(itertools.combinations(group_IV, 2))  # group-IV binaries\n",
    "    \n",
    "    pairs += list(itertools.product(group_IV, VI))     # IV–VI (e.g., SnSe, GeS)\n",
    "    pairs += list(itertools.product([\"Cu\",\"Ag\"], VI))  # I–VI (for chalco precursors)\n",
    "    seen, ordered = set(), []\n",
    "    for p in pairs:\n",
    "        key = tuple(sorted(p))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            ordered.append(tuple(p))\n",
    "    return ordered\n",
    "\n",
    "def build_ternaries():\n",
    "    s = set()\n",
    "    # III–V–N\n",
    "    for a, b in itertools.product(III, V):\n",
    "        s.add(tuple(sorted((a, b, \"N\"))))\n",
    "    # III–V–VI\n",
    "    for a, b, c in itertools.product(III, V, VI):\n",
    "        s.add(tuple(sorted((a, b, c))))\n",
    "    \n",
    "    for a, b, c in itertools.product([\"Cu\",\"Ag\"], III, VI):  # I–III–VI2 chalcopyrites\n",
    "        s.add(tuple(sorted((a, b, c))))\n",
    "    for a, b, c in itertools.product(II, group_IV, V):       # II–IV–V2 (e.g., ZnSiAs2)\n",
    "        s.add(tuple(sorted((a, b, c))))\n",
    "    out = [t for t in s if isinstance(t, (tuple, list)) and len(t) == 3]\n",
    "    return sorted(out)\n",
    "\n",
    "pairs = build_pairs()\n",
    "ternaries = build_ternaries()\n",
    "\n",
    "# setting fetch helper using 'num_elements' filter; suppressing progress per call\n",
    "def fetch_elements_combo(required_elements, num_elements=2, chunk_size=400, max_chunks=100):\n",
    "    out = []\n",
    "    with MPRester(os.environ[\"MP_API_KEY\"]) as mpr:\n",
    "        for _ in range(max_chunks):\n",
    "            with suppress_progress():\n",
    "                docs = mpr.materials.summary.search(\n",
    "                    elements=list(required_elements),\n",
    "                    num_elements=num_elements,\n",
    "                    fields=FIELDS,\n",
    "                    num_chunks=1,\n",
    "                    chunk_size=chunk_size\n",
    "                )\n",
    "            if not docs:\n",
    "                break\n",
    "            out.extend(docs)\n",
    "            if len(docs) < chunk_size:\n",
    "                break\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# collection + band gap filter\n",
    "# ----------------------------\n",
    "\n",
    "seen = set()\n",
    "collected = []\n",
    "\n",
    "def add_docs(docs):\n",
    "    added = 0\n",
    "    for d in docs:\n",
    "        mid = getattr(d, \"material_id\", None)\n",
    "        if not mid or mid in seen:\n",
    "            continue\n",
    "        Eg = getattr(d, \"band_gap\", None)\n",
    "        if Eg is None or not (EG_MIN < Eg < EG_MAX):\n",
    "            continue\n",
    "        collected.append(d)\n",
    "        seen.add(mid)\n",
    "        added += 1\n",
    "    return added\n",
    "\n",
    "# ----------------------------\n",
    "# run: binaries first, then ternaries\n",
    "# ----------------------------\n",
    "\n",
    "for a, b in pairs:\n",
    "    add_docs(fetch_elements_combo([a, b], num_elements=2, chunk_size=400, max_chunks=30))\n",
    "    if len(collected) >= TARGET_N:\n",
    "        break\n",
    "\n",
    "if len(collected) < TARGET_N:\n",
    "    for a, b, c in ternaries:\n",
    "        add_docs(fetch_elements_combo([a, b, c], num_elements=3, chunk_size=400, max_chunks=25))\n",
    "        if len(collected) >= TARGET_N:\n",
    "            break\n",
    "\n",
    "print(f\"Collected {len(collected)} unique semiconductor-like entries (binaries/ternaries).\")\n",
    "\n",
    "# ----------------------------\n",
    "# serialization helpers (JSON-safe)\n",
    "# ----------------------------\n",
    "\n",
    "def to_symbol_list(elems):\n",
    "    # converting Element objects -> symbols; leaving strings untouched\n",
    "    try:\n",
    "        if elems and not isinstance(elems[0], str):\n",
    "            return [getattr(e, \"symbol\", str(e)) for e in elems]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return elems\n",
    "\n",
    "def structure_to_dict(s):\n",
    "    # converting Structure to plain dict; handling dict/None gracefully\n",
    "    if s is None:\n",
    "        return None\n",
    "    if isinstance(s, dict):\n",
    "        return s\n",
    "    # try as_dict(), else try to_json() -> dict\n",
    "    try:\n",
    "        return s.as_dict()\n",
    "    except Exception:\n",
    "        try:\n",
    "            return json.loads(s.to_json())\n",
    "        except Exception:\n",
    "            return str(s)  # last resort: stringify\n",
    "\n",
    "class MPJSONEncoder(json.JSONEncoder):\n",
    "    # handling pymatgen Element, numpy types, and MSONable objects\n",
    "    def default(self, o):\n",
    "        try:\n",
    "            from pymatgen.core.periodic_table import Element\n",
    "            if isinstance(o, Element):\n",
    "                return o.symbol\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            import numpy as np\n",
    "            if isinstance(o, (np.integer,)):\n",
    "                return int(o)\n",
    "            if isinstance(o, (np.floating,)):\n",
    "                return float(o)\n",
    "            if isinstance(o, (np.ndarray,)):\n",
    "                return o.tolist()\n",
    "        except Exception:\n",
    "            pass\n",
    "        if hasattr(o, \"as_dict\"):\n",
    "            return o.as_dict()\n",
    "        return super().default(o)\n",
    "\n",
    "# ----------------------------\n",
    "# material dicts\n",
    "# ----------------------------\n",
    "\n",
    "def doc_to_plain(d):\n",
    "    base = {\n",
    "        \"material_id\": d.material_id,\n",
    "        \"formula_pretty\": d.formula_pretty,\n",
    "        \"elements\": to_symbol_list(d.elements),\n",
    "        \"nelements\": d.nelements,\n",
    "        \"formation_energy_per_atom\": d.formation_energy_per_atom,\n",
    "        \"energy_above_hull\": d.energy_above_hull,\n",
    "        \"is_stable\": d.is_stable,\n",
    "        \"band_gap\": d.band_gap,\n",
    "        \"is_metal\": d.is_metal,\n",
    "        \"density\": d.density,\n",
    "    }\n",
    "    if INCLUDE_STRUCTURE and getattr(d, \"structure\", None) is not None:\n",
    "        base[\"structure\"] = structure_to_dict(d.structure)\n",
    "    return base\n",
    "\n",
    "plain = [doc_to_plain(d) for d in collected]\n",
    "\n",
    "# ----------------------------\n",
    "# write: JSON (full) + CSV (composition-only)\n",
    "# ----------------------------\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "suffix = \"with_struct\" if INCLUDE_STRUCTURE else \"comp_only\"\n",
    "\n",
    "# writing full json.gz (safe encoder)\n",
    "json_path = f\"data/semiconductors_full_{suffix}.json.gz\"\n",
    "with gzip.open(json_path, \"wt\") as f:\n",
    "    json.dump(plain, f, cls=MPJSONEncoder)\n",
    "print(f\"✅ saved JSON → {json_path} (records: {len(plain)})\")\n",
    "\n",
    "# writing CSV.gz (composition-only; omitting 'structure')\n",
    "csv_cols = [\"material_id\",\"formula_pretty\",\"elements\",\"nelements\",\n",
    "            \"formation_energy_per_atom\",\"energy_above_hull\",\"is_stable\",\n",
    "            \"band_gap\",\"is_metal\",\"density\"]\n",
    "df = pd.DataFrame([{k: rec.get(k) for k in csv_cols} for rec in plain])\n",
    "csv_path = f\"data/semiconductors_comp_only.csv.gz\"\n",
    "df.to_csv(csv_path, index=False, compression=\"gzip\")\n",
    "print(f\"✅ saved CSV  → {csv_path} (rows: {len(df)})\")\n",
    "\n",
    "# quick peek\n",
    "print(df.shape)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22527932",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4712c5",
   "metadata": {},
   "source": [
    "**a. Composition pipeline**    \n",
    "\n",
    "load raw comp dataset → standardize columns/types → filter valid rows → attach Composition objects → featurize elemental stats → assemble X,y → persist features → quick QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7f1a1",
   "metadata": {},
   "source": [
    "*a.1: (load + standardize + initial filtering)*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8ccceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ cleaned comp dataset → data/comp_raw_clean.csv.gz | rows=1285\n",
      "  material_id formula_pretty elements  nelements  band_gap\n",
      "0  mp-1244872             BN   [B, N]          2    1.5766\n",
      "1  mp-1244917             BN   [B, N]          2    1.1992\n",
      "2  mp-1244943             BN   [B, N]          2    1.1885\n",
      "3  mp-1244991             BN   [B, N]          2    1.5503\n",
      "4  mp-1245193             BN   [B, N]          2    0.2781\n",
      "binaries: 619 | ternaries: 666 \n"
     ]
    }
   ],
   "source": [
    "# setting paths\n",
    "RAW_CSV = \"data/semiconductors_comp_only.csv.gz\"\n",
    "CLEAN_CSV = \"data/comp_raw_clean.csv.gz\"\n",
    "\n",
    "# importing libs\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# reading comp-only dataset  \n",
    "df = pd.read_csv(RAW_CSV)\n",
    "\n",
    "# standardizing columns expected downstream\n",
    "required_cols = [\"material_id\",\"formula_pretty\",\"elements\",\"nelements\",\"band_gap\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"missing columns in raw file: {missing}\")\n",
    "\n",
    "# converting 'elements' column from string repr -> python list[str]\n",
    "def _parse_elems(x):\n",
    "    # handling NaN or already-a-list\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    try:\n",
    "        obj = ast.literal_eval(x)\n",
    "        if isinstance(obj, list):\n",
    "            # enforcing strings (symbols) only\n",
    "            return [str(e) for e in obj]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallback: split on non-letters if a weird string sneaks in\n",
    "    return [tok for tok in str(x).replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(\",\") if tok.strip()]\n",
    "\n",
    "df[\"elements\"] = df[\"elements\"].apply(_parse_elems)\n",
    "\n",
    "# dropping bad rows (no elements list, no formula, invalid Eg)\n",
    "df = df.dropna(subset=[\"formula_pretty\",\"elements\",\"band_gap\"])\n",
    "df = df[df[\"band_gap\"].between(0, 4, inclusive=\"neither\")]\n",
    "\n",
    "# dropping duplicate materials (keeping the first)\n",
    "if \"material_id\" in df.columns:\n",
    "    df = df.drop_duplicates(subset=[\"material_id\"])\n",
    "\n",
    "# saving cleaned raw for the next substep\n",
    "df.to_csv(CLEAN_CSV, index=False, compression=\"gzip\")\n",
    "\n",
    "# counting binaries/ternaries\n",
    "n_bin  = int((df[\"nelements\"] == 2).sum())\n",
    "n_ter  = int((df[\"nelements\"] == 3).sum())\n",
    "\n",
    "\n",
    "# quick peek\n",
    "print(f\"✅ cleaned comp dataset → {CLEAN_CSV} | rows={len(df)}\")\n",
    "print(df[[\"material_id\",\"formula_pretty\",\"elements\",\"nelements\",\"band_gap\"]].head(5))\n",
    "print(f\"binaries: {n_bin} | ternaries: {n_ter} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421caae",
   "metadata": {},
   "source": [
    "*a.2: composition prep*  \n",
    "\n",
    "load raw comp dataset → attach Composition objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af6e01e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ comp cache saved → data/comp_cache.csv.gz | rows=1285\n",
      "  material_id formula_pretty  nelements  band_gap           comp_el_amt\n",
      "0  mp-1244872             BN          2    1.5766  {'B': 1.0, 'N': 1.0}\n",
      "1  mp-1244917             BN          2    1.1992  {'B': 1.0, 'N': 1.0}\n",
      "2  mp-1244943             BN          2    1.1885  {'B': 1.0, 'N': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# setting paths\n",
    "CLEAN_CSV = \"data/comp_raw_clean.csv.gz\"\n",
    "COMP_CACHE_CSV = \"data/comp_cache.csv.gz\"\n",
    "\n",
    "# importing libs\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymatgen.core.composition import Composition\n",
    "\n",
    "# loading cleaned dataset\n",
    "df = pd.read_csv(CLEAN_CSV)\n",
    "\n",
    "# setting safe Composition parser\n",
    "def to_comp_safe(formula: str):\n",
    "    try:\n",
    "        return Composition(str(formula))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "df[\"comp_obj\"] = df[\"formula_pretty\"].apply(to_comp_safe)\n",
    "df[\"comp_ok\"] = df[\"comp_obj\"].notna()\n",
    "\n",
    "# dropping rows that failed to parse\n",
    "n_fail = int((~df[\"comp_ok\"]).sum())\n",
    "if n_fail:\n",
    "    print(f\"dropping {n_fail} rows that failed Composition() parsing\")\n",
    "df = df[df[\"comp_ok\"]].copy()\n",
    "\n",
    "# setting element→amount maps (keys are symbols, values are floats)\n",
    "def el_amt_map(c: Composition):\n",
    "    d = c.get_el_amt_dict()  # keys already symbols in current pymatgen\n",
    "    return {str(k): float(v) for k, v in d.items()}\n",
    "\n",
    "df[\"comp_el_amt\"]   = df[\"comp_obj\"].apply(el_amt_map)\n",
    "df[\"comp_mson\"]     = df[\"comp_obj\"].apply(lambda c: c.as_dict())\n",
    "\n",
    "# serializing dicts for CSV\n",
    "df[\"comp_el_amt_json\"] = df[\"comp_el_amt\"].apply(json.dumps)\n",
    "df[\"comp_mson_json\"]   = df[\"comp_mson\"].apply(json.dumps)\n",
    "\n",
    "# saving compact cache for next substep\n",
    "cache_cols = [\n",
    "    \"material_id\",\"formula_pretty\",\"elements\",\"nelements\",\"band_gap\",\n",
    "    \"comp_el_amt_json\",\"comp_mson_json\"\n",
    "]\n",
    "df[cache_cols].to_csv(COMP_CACHE_CSV, index=False, compression=\"gzip\")\n",
    "\n",
    "# quick QA\n",
    "print(f\"✅ comp cache saved → {COMP_CACHE_CSV} | rows={len(df)}\")\n",
    "print(df[[\"material_id\",\"formula_pretty\",\"nelements\",\"band_gap\",\"comp_el_amt\"]].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2553b10",
   "metadata": {},
   "source": [
    "*a.3: : featurize elemental stats*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8a70637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ composition features computed | rows=1285 | cols=31\n",
      "  material_id  nelements  H_composition  max_elem_frac  Z_mean  Z_std  X_mean  \\\n",
      "0  mp-1244872          2       0.693147            0.5     6.0    1.0    2.54   \n",
      "1  mp-1244917          2       0.693147            0.5     6.0    1.0    2.54   \n",
      "2  mp-1244943          2       0.693147            0.5     6.0    1.0    2.54   \n",
      "\n",
      "   X_std  mass_mean  mass_std  band_gap  \n",
      "0    0.5   12.40885   1.59785    1.5766  \n",
      "1    0.5   12.40885   1.59785    1.1992  \n",
      "2    0.5   12.40885   1.59785    1.1885  \n"
     ]
    }
   ],
   "source": [
    "# setting paths\n",
    "COMP_CACHE_CSV = \"data/comp_cache.csv.gz\"\n",
    "\n",
    "from pymatgen.core import Element\n",
    "import numpy as np\n",
    "\n",
    "# loading composition cache\n",
    "df = pd.read_csv(COMP_CACHE_CSV)\n",
    "\n",
    "# parsing element→amount dicts (unnormalized), then making fractions\n",
    "def parse_el_amt(s):\n",
    "    try:\n",
    "        d = json.loads(s)\n",
    "        # forcing keys as symbols, values as float\n",
    "        d = {str(k): float(v) for k, v in d.items()}\n",
    "        total = sum(d.values())\n",
    "        if total <= 0:\n",
    "            return None, None\n",
    "        fracs = {k: v / total for k, v in d.items()}\n",
    "        return d, fracs\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "el_amt_fracs = df[\"comp_el_amt_json\"].apply(parse_el_amt)\n",
    "df[\"el_amt\"]   = [t[0] for t in el_amt_fracs]\n",
    "df[\"el_fracs\"] = [t[1] for t in el_amt_fracs]\n",
    "df = df.dropna(subset=[\"el_fracs\"]).reset_index(drop=True)\n",
    "\n",
    "# setting elemental properties to aggregate\n",
    "# using: atomic number (Z), Pauling electronegativity (X), atomic mass, Mendeleev number, atomic radius\n",
    "PROP_FUNCS = {\n",
    "    \"Z\":               lambda e: e.Z,\n",
    "    \"X\":               lambda e: e.X,                 # may be None for some elements\n",
    "    \"mass\":            lambda e: float(e.atomic_mass) if e.atomic_mass is not None else np.nan,\n",
    "    \"mendeleev_no\":    lambda e: e.mendeleev_no,\n",
    "    \"atomic_radius\":   lambda e: e.atomic_radius,     # may be None\n",
    "}\n",
    "\n",
    "# setting helper to get numeric array for a property given fraction dict\n",
    "def prop_values_and_weights(fracs: dict, prop_key: str):\n",
    "    vals, wts = [], []\n",
    "    fn = PROP_FUNCS[prop_key]\n",
    "    for sym, w in fracs.items():\n",
    "        try:\n",
    "            e = Element(sym)\n",
    "            v = fn(e)\n",
    "        except Exception:\n",
    "            v = np.nan\n",
    "        vals.append(np.nan if v is None else float(v))\n",
    "        wts.append(float(w))\n",
    "    return np.array(vals, dtype=float), np.array(wts, dtype=float)\n",
    "\n",
    "# setting weighted stats (fractions already sum to 1)\n",
    "def weighted_stats(vals: np.ndarray, wts: np.ndarray):\n",
    "    # drop NaNs in vals (adjust weights)\n",
    "    mask = ~np.isnan(vals)\n",
    "    if not np.any(mask):\n",
    "        return dict(mean=np.nan, std=np.nan, vmin=np.nan, vmax=np.nan, vrange=np.nan)\n",
    "    v = vals[mask]\n",
    "    w = wts[mask]\n",
    "    w = w / w.sum()\n",
    "    m = np.sum(w * v)\n",
    "    var = np.sum(w * (v - m) ** 2)\n",
    "    s = math.sqrt(var)\n",
    "    vmin, vmax = float(np.min(v)), float(np.max(v))\n",
    "    return dict(mean=float(m), std=float(s), vmin=vmin, vmax=vmax, vrange=float(vmax - vmin))\n",
    "\n",
    "# setting composition diversity metrics\n",
    "def comp_entropy(fracs: dict):\n",
    "    p = np.array(list(fracs.values()), dtype=float)\n",
    "    return float(-np.sum(p * np.log(p)))  # natural log\n",
    "\n",
    "def comp_max_fraction(fracs: dict):\n",
    "    return float(max(fracs.values()))\n",
    "\n",
    "# computing features\n",
    "feat_rows = []\n",
    "for i, row in df.iterrows():\n",
    "    fr = row[\"el_fracs\"]\n",
    "    feats = {\n",
    "        \"material_id\": row[\"material_id\"],\n",
    "        \"nelements\": row[\"nelements\"],\n",
    "        \"H_composition\": comp_entropy(fr),\n",
    "        \"max_elem_frac\": comp_max_fraction(fr),\n",
    "    }\n",
    "    for pk in PROP_FUNCS.keys():\n",
    "        vals, wts = prop_values_and_weights(fr, pk)\n",
    "        st = weighted_stats(vals, wts)\n",
    "        feats[f\"{pk}_mean\"]   = st[\"mean\"]\n",
    "        feats[f\"{pk}_std\"]    = st[\"std\"]\n",
    "        feats[f\"{pk}_min\"]    = st[\"vmin\"]\n",
    "        feats[f\"{pk}_max\"]    = st[\"vmax\"]\n",
    "        feats[f\"{pk}_range\"]  = st[\"vrange\"]\n",
    "    feat_rows.append(feats)\n",
    "\n",
    "comp_feats = pd.DataFrame(feat_rows)\n",
    "\n",
    "# joining back light metadata for convenience\n",
    "comp_feats = comp_feats.merge(df[[\"material_id\",\"formula_pretty\",\"band_gap\"]], on=\"material_id\", how=\"left\")\n",
    "\n",
    "# quick QA\n",
    "print(f\"✅ composition features computed | rows={len(comp_feats)} | cols={comp_feats.shape[1]}\")\n",
    "print(comp_feats.head(3)[[\n",
    "    \"material_id\",\"nelements\",\"H_composition\",\"max_elem_frac\",\n",
    "    \"Z_mean\",\"Z_std\",\"X_mean\",\"X_std\",\"mass_mean\",\"mass_std\",\"band_gap\"\n",
    "]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359d857",
   "metadata": {},
   "source": [
    "*a.4: assemble X, y*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09ad6195",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# expecting 'comp_feats' from previous substep; failing fast if missing\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomp_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# setting paths\n",
    "FEATS_ALL_PATH   = \"data/comp_X_all.parquet\"\n",
    "TARGET_ALL_PATH  = \"data/comp_y_all.parquet\"\n",
    "XTR_PATH, XVA_PATH = \"data/comp_X_train.parquet\", \"data/comp_X_valid.parquet\"\n",
    "YTR_PATH, YVA_PATH = \"data/comp_y_train.parquet\", \"data/comp_y_valid.parquet\"\n",
    "\n",
    "# importing libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# expecting 'comp_feats' from previous substep; failing fast if missing\n",
    "if \"comp_feats\" not in globals():\n",
    "    raise RuntimeError(\"comp_feats not found. Run the previous substep to build composition features.\")\n",
    "\n",
    "# selecting feature columns (numerics only, drop metadata/target)\n",
    "meta_cols = {\"material_id\",\"formula_pretty\",\"band_gap\"}\n",
    "num_df = comp_feats.drop(columns=[c for c in meta_cols if c in comp_feats.columns])\n",
    "num_df = num_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# dropping columns with too many NaNs (setting threshold = 30%)\n",
    "na_ratio = num_df.isna().mean()\n",
    "drop_cols = list(na_ratio[na_ratio > 0.30].index)\n",
    "if drop_cols:\n",
    "    print(f\"dropping {len(drop_cols)} cols with >30% NaNs\")\n",
    "    num_df = num_df.drop(columns=drop_cols)\n",
    "\n",
    "# replacing infs -> NaN then imputing with column medians\n",
    "num_df = num_df.replace([np.inf, -np.inf], np.nan)\n",
    "medians = num_df.median(axis=0, numeric_only=True)\n",
    "num_df = num_df.fillna(medians)\n",
    "\n",
    "# enforcing float32 for compactness/speed\n",
    "X = num_df.astype(np.float32)\n",
    "y = comp_feats[\"band_gap\"].astype(np.float32)\n",
    "\n",
    "# saving full matrices (for later reuse)\n",
    "X.to_parquet(FEATS_ALL_PATH, index=False)\n",
    "y.to_frame(\"band_gap\").to_parquet(TARGET_ALL_PATH, index=False)\n",
    "\n",
    "# creating a simple train/valid split (setting random_state for reproducibility)\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# saving split\n",
    "X_tr.to_parquet(XTR_PATH, index=False)\n",
    "X_va.to_parquet(XVA_PATH, index=False)\n",
    "y_tr.to_frame(\"band_gap\").to_parquet(YTR_PATH, index=False)\n",
    "y_va.to_frame(\"band_gap\").to_parquet(YVA_PATH, index=False)\n",
    "\n",
    "# quick QA\n",
    "print(f\"✅ X_all: {X.shape} | y_all: {y.shape}\")\n",
    "print(f\"   train: {X_tr.shape} / valid: {X_va.shape} | features: {X.shape[1]}\")\n",
    "print(f\"   dropped_cols({len(drop_cols)}): {drop_cols[:8]}{' ...' if len(drop_cols)>8 else ''}\")\n",
    "print(f\"   y stats → min={float(y.min()):.3f}, median={float(y.median()):.3f}, max={float(y.max()):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "materials-mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
